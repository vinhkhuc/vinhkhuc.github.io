---
layout: post
mathjax: true
comments: true
title:  "Recurrent Neural Networks"
excerpt: "This post will cover the ideas behind Recurrent Neural Networks and its improved variation (Long Short-Term Memory), as well as some aspects of its implementation."
date:   2015-03-11 23:53:10
---

### Introduction
**Recurrent Neural Network** (**RNN**) is a neural network which has at least one feedback loop. RNN defines a non-linear 
dynamic system which can learn the mapping from input sequences to output sequences. RNN is considered [the deepest of 
all neural networks](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/cp43748). 
RNN has many forms and the simplest one is a Multilayer Perceptron where previous hidden nodes feed back into the network.

<div class="center-figure">
    <img src="/assets/2015-03-11-rnn-lstm/rnn-basic.png" width="50%" height="50%">
    <p><em>Figure 1: Recurrent Neural Network</em></p>
</div>

For language modeling (i.e. predicting the next word based on previous words), RNN significantly outperforms the standard 
N-gram-based methods. 

However, RNN is difficult to train since it suffers from the vanishing / exploding gradient effects
where error signals become smaller and smaller (in the vanishing case) / larger and larger (in the exploding case) when 
back-propagating errors from output layer back to input layer. 

<div class="center-figure">
    <img src="/assets/2015-03-11-rnn-lstm/rnn-unit.png" width="30%" height="30%">
    <p><em>Figure 2: RNN Unit (adapted from [1])</em></p>
</div>

$$
\begin{align}
h_t &= \sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\
y_t &= \sigma(W_{hy} h_t + b_y)
\end{align}
$$

where $$\sigma$$ is sigmoid function $$\sigma(x) = \frac{1}{1+e^{-x}}$$.

**Long Short-Term Memory** (**LSTM**) is a variant of RNN which was proposed to resolve the vanishing / exploding gradient problems. 
LSTM has memory nodes that helps the network learn when to forget previous hidden states and when to update hidden states 
given new input. LSTM has been used very successfully in handwriting recognition.  

<div class="center-figure">
    <img src="/assets/2015-03-11-rnn-lstm/lstm-unit.png" width="65%" height="65%">
    <p><em>Figure 3: LSTM Unit (adapted from [1])</em></p>
</div>

$$
\begin{align}
i_t &= \sigma(W_{wi} x_t + W_{hi} h_{t-1} + b_i)  \\
f_t &= \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)  \\
o_t &= \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)  \\
g_t &= \varphi(W_{xc} x_t + W_{hc} h_{t-1} + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t          \\
h_t &= o_t \odot \varphi(c_t)
\end{align}
$$

where $$\sigma$$ is sigmoid function, $$\varphi$$ is tanh function, and $$\odot$$ is 
element-wise product.

### Backpropagation
An RNN is actually a feed-forward neutral network where RNN's sequence length is its depth. For simplicity, here we will 
derive backpropagation steps to train the vanilla RNN (Figure 2) for a classification task.

Given a training data set $$D = \{(X_i, Y_i)\}_{i=1}^N$$, where $$X_i$$ and $$Y_i$$ are two sequences with length T, 
the loss function is a log-loss, i.e. cross entropy between the ground truth sequence $$y$$ and the predicted sequence $$\hat{y}$$: 

$$L(Y, \hat{Y}) = -\sum_{i=1}^Ty_t \log\hat{y_t}$$

The forward pass is pretty straightforward as shown in Figure 2. The backward pass is quite tricky due to the link between
$$h_{t-1}$$ and $$h_t$$. Before diving into that, we want to require the loss function as
$$L(Y, \hat{Y}) = -\sum_{i=1}^TL_t$$, where $$L_t = -y_t \log\hat{y_t}$$.

We will backpropagate the error starting at $$t = T$$ back to $$t = 1$$:

1) Backpropagate to $$y_t$$:



### References
[1] Jeff Donahue et al. Long-term Recurrent Convolutional Networks for Visual Recognition and Description ([URL](//arxiv.org/abs/1411.4389)).