---
layout: post
mathjax: true
comments: true
title:  "Recurrent Neural Networks"
excerpt: "This post will cover the ideas behind Recurrent Neural Networks and its improved variation (Long Short-Term Memory), as well as some aspects of its implementation."
date:   2015-03-11 23:53:10
---
(The content of this post reuses some materials from the paper [Long-term Recurrent Convolutional Networks for Visual 
Recognition and Description](//arxiv.org/abs/1411.4389))

### Introduction
Recurrent Neural Networks (RNN)

Long Short-Term Memory (LSTM)

### RNN Unit
<div style="text-align:center;">
    <img src="/assets/2015-03-11-rnn-lstm/rnn-unit.png" width="30%" height="30%">
    <p><em>Figure 1: RNN Unit</em></p>
</div>

$$
h_t = g(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\
z_t = g(W_{hz} h_t + b_z)
$$

### LSTM Unit
<div style="text-align:center;">
    <img src="/assets/2015-03-11-rnn-lstm/lstm-unit.png" width="65%" height="65%">
    <p><em>Figure 2: LSTM Unit</em></p>
</div>

$$
i_t = \sigma(W_{wi} x_t + W_{hi} h_{t-1} + b_i) \\
f_t = \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f) \\
o_t = \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o) \\
g_t = \phi(W_{xc} x_t + W_{hc} h_{t-1} + b_c)   \\
c_t = f_t \odot c_{t-1} + i_t \odot g_t         \\
h_t = o_t \odot \phi(c_t)
$$