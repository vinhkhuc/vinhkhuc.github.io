---
layout: post
mathjax: true
comments: true
title:  "Recurrent Neural Networks"
excerpt: "This post will cover the ideas behind Recurrent Neural Networks and its improved variation (Long Short-Term Memory), as well as some aspects of its implementation."
date:   2015-03-11 23:53:10
---

### Introduction
**Recurrent Neural Network** (**RNN**) is a neural network which has at least one feedback loop. RNN defines a non-linear 
dynamic system which can learn the mapping from input sequences to output sequences. RNN is considered [the deepest of 
all neural networks](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/cp43748). 
RNN has many forms and the simplest one is a Multilayer Perceptron where previous hidden nodes feed back into the network.

<div class="center-figure">
    <img src="/assets/2015-03-11-rnn-lstm/rnn-basic.png" width="50%" height="50%">
    <p><em>Figure 1: Simple RNN</em></p>
</div>

For language modeling (i.e. predicting the next word based on previous words), RNN significantly outperforms the standard 
N-gram-based methods. 

However, RNN is difficult to train since it suffers from the vanishing / exploding gradient effects
where error signals become smaller and smaller (in the vanishing case) / larger and larger (in the exploding case) when 
back-propagating errors from output layer back to input layer. 

<div class="center-figure">
    <img src="/assets/2015-03-11-rnn-lstm/rnn-unit.png" width="30%" height="30%">
    <p><em>Figure 2: RNN Unit (adapted from [1])</em></p>
</div>

$$
\begin{align}
h_t &= \sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\
y_t &= \sigma(W_{hy} h_t + b_y)
\end{align}
$$

where $$\sigma$$ is sigmoid function $$\sigma(x) = \frac{1}{1+e^{-x}}$$.

**Long Short-Term Memory** (**LSTM**) is a variant of RNN which was proposed to resolve the vanishing / exploding gradient problems. 
LSTM has memory nodes that helps the network learn when to forget previous hidden states and when to update hidden states 
given new input. LSTM has been used very successfully in handwriting recognition.  

<div class="center-figure">
    <img src="/assets/2015-03-11-rnn-lstm/lstm-unit.png" width="65%" height="65%">
    <p><em>Figure 3: LSTM Unit (adapted from [1])</em></p>
</div>

$$
\begin{align}
i_t &= \sigma(W_{wi} x_t + W_{hi} h_{t-1} + b_i)  \\
f_t &= \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)  \\
o_t &= \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)  \\
g_t &= \varphi(W_{xc} x_t + W_{hc} h_{t-1} + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t          \\
h_t &= o_t \odot \varphi(c_t)
\end{align}
$$

where $$\sigma$$ is sigmoid function, $$\varphi$$ is tanh function, and $$\odot$$ is 
element-wise product.

### Backpropagation
TODO

### References
[1] Jeff Donahue et al. Long-term Recurrent Convolutional Networks for Visual Recognition and Description. [Link](//arxiv.org/abs/1411.4389).